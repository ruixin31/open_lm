# HELM scenarios.

entries: [
  # NarrativeQA
  {description: "narrative_qa:model={model_name}", priority: 1}

  # NaturalQuestions
  {description: "natural_qa:model={model_name},mode=openbook_longans", priority: 1}
  {description: "natural_qa:model={model_name},mode=closedbook", priority: 1}

  # OpenbookQA
  {description: "commonsense:model={model_name},dataset=openbookqa,method=multiple_choice_joint", priority: 1}

  # MMLU
  {description: "mmlu:model={model_name},subject=abstract_algebra", priority: 2}
  {description: "mmlu:model={model_name},subject=college_chemistry", priority: 2}
  {description: "mmlu:model={model_name},subject=computer_security", priority: 2}
  {description: "mmlu:model={model_name},subject=econometrics", priority: 2}
  {description: "mmlu:model={model_name},subject=us_foreign_policy", priority: 2}

  # MATH
  {description: "math:model={model_name},subject=number_theory,level=1,use_chain_of_thought=True", priority: 2}
  {description: "math:model={model_name},subject=intermediate_algebra,level=1,use_chain_of_thought=True", priority: 2}
  {description: "math:model={model_name},subject=algebra,level=1,use_chain_of_thought=True", priority: 2}
  {description: "math:model={model_name},subject=prealgebra,level=1,use_chain_of_thought=True", priority: 2}
  {description: "math:model={model_name},subject=geometry,level=1,use_chain_of_thought=True", priority: 2}
  {description: "math:model={model_name},subject=counting_and_probability,level=1,use_chain_of_thought=True", priority: 2}
  {description: "math:model={model_name},subject=precalculus,level=1,use_chain_of_thought=True", priority: 2}





  # GSM
  {description: "gsm:model={model_name}", priority: 2}

  # HumanEval
  {description: "code:model={model_name},dataset=humaneval", priority: 2}

  # # LegalBench
  # {description: "legalbench:model={model_name},subset=abercrombie", priority: 2}
  # {description: "legalbench:model={model_name},subset=corporate_lobbying", priority: 2}
  # {description: "legalbench:model={model_name},subset=international_citizenship_questions", priority: 2}
  # {description: "legalbench:model={model_name},subset=function_of_decision_section", priority: 2}
  # {description: "legalbench:model={model_name},subset=proa", priority: 2}

  # MedQA
  {description: "med_qa:model={model_name}", priority: 2}

  # WMT14
  {description: "wmt_14:language_pair=cs-en,model={model_name}", priority: 2}
  {description: "wmt_14:language_pair=de-en,model={model_name}", priority: 2}
  {description: "wmt_14:language_pair=fr-en,model={model_name}", priority: 2}
  {description: "wmt_14:language_pair=hi-en,model={model_name}", priority: 2}
  {description: "wmt_14:language_pair=ru-en,model={model_name}", priority: 2}
  

  # Perplextiy ranking evals
]